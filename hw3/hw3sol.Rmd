---
title: "Biostat 200C Homework 3"
subtitle: Due May 10 @ 11:59PM
output: 
  html_document:
    toc: true
    toc_depth: 4
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please upload both Rmd and html files to Bruinlearn
by the deadline.

## Q1. Concavity of Poisson regression log-likelihood

Let $Y_1,\ldots,Y_n$ be independent random variables with
$Y_i \sim \text{Poisson}(\mu_i)$ and
$\log \mu_i = \mathbf{x}_i^T \boldsymbol{\beta}$, $i = 1,\ldots,n$.

### Q1.1

Write down the log-likelihood function.

**Answer:**

$f(y_i;\mu_i)=e^{-\mu_i}\frac{\mu_i^{y_i}}{y_i!}$

$L(\mu_i)=\prod_{i=1}^{n}f(y_i;\mu_i)$

$$
l(\mu_i)=\sum_{i=1}^{n}log(f(y_i;\mu_i))\\
=\sum_{i=1}^{n}(-\mu_i+y_i\log(\mu_i)-\log(y_i!))\\
=\sum_{i=1}^{n}(-e^{\mathbf{x}_i^T \boldsymbol{\beta}}+y_i\mathbf{x}_i^T \boldsymbol{\beta}-\log(y_i!))\\
$$

### Q1.2

Derive the gradient vector and Hessian matrix of the log-likelhood
function with respect to the regression coefficients
$\boldsymbol{\beta}$.

**Answer:**

Note $\boldsymbol{x}_i^T$ is a row vector of $\boldsymbol{X}$ instead of
transpose of its first column vector

$\nabla_{\beta} l(\mu_i)=\sum_{i=1}^{n}(-e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}\boldsymbol{x}^T_i+y_i\mathbf{x}_i^T)$

$\nabla_{\beta}^2 l(\mu_i)=\sum_{i=1}^{n}(-e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i)^T\boldsymbol{x}_i^T)$

### Q1.3

Show that the log-likelihood function of the log-linear model is a
concave function in regression coefficients $\boldsymbol{\beta}$. (Hint:
show that the negative Hessian is a positive semidefinite matrix.)

**Answer:**

$\sum_{i=1}^{n}(e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i)^T\boldsymbol{x}_i^T)=\sum_{i=1}^{n}(\boldsymbol{a}^Te^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i)^T\boldsymbol{x}_i^T\boldsymbol{a})=\sum_{i=1}^{n}(e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i\boldsymbol{a})^T(\boldsymbol{x}_i^T\boldsymbol{a}))=\sum_{i=1}^{n}(e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i\boldsymbol{a})^2)\geq 0, \text{ for all } \boldsymbol{a\neq 0}$

By energy-based definition,

$\sum_{i=1}^{n}(e^{\boldsymbol{x}^T_i\boldsymbol{\beta}}(\boldsymbol{x}^T_i)^T\boldsymbol{x}_i^T)\succcurlyeq 0$

Therefore, the log-likelihood function of the log-linear model is a
concave function in regression coefficients $\boldsymbol{\beta}$.

### Q1.4

Show that for the fitted values $\widehat{\mu}_i$ from maximum
likelihood estimates

$$
\sum_i \widehat{\mu}_i = \sum_i y_i.
$$

Therefore the deviance reduces to

$$
D = 2 \sum_i y_i \log \frac{y_i}{\widehat{\mu}_i}.
$$

**Answer:**

$l(\mu_i)=\sum_{i=1}^{n}(-\mu_i+y_i\log(\mu_i)-\log(y_i!))$

$\frac{d}{d\mu_i}l(\mu_i)=\sum_{i=1}^{n}(-1+\frac{y_i}{\mu_i})=0$

$\sum_{i=1}^{n}\widehat{u}_i=\sum_{i=1}^{n}y_i$

Therefore,

$$
D = 2\sum_i[y_i\log (y_i)-y_i]-2\sum_i[y_i\log(\widehat{\mu}_i)-\widehat{\mu}_i]\\
=2\sum_i[y_i\log(y_i/\widehat{\mu})]-(y_i-\widehat{\mu}_i)]\\
=2 \sum_i y_i \log \frac{y_i}{\widehat{\mu}_i}
$$

## Q2. Show negative binomial distribution mean and variance

Recall the probability mass function of negative binomial distribution
is

$$
\mathbb{P}(Y = y) = \binom{y + r - 1}{r - 1} (1 - p)^r p^y, \quad y = 0, 1, \ldots
$$

Show $\mathbb{E}Y = \mu = rp / (1 - p)$ and
$\operatorname{Var} Y = r p / (1 - p)^2$.

**Answerï¼š**

Recall $logM_Y(t)=r{log(1-p)-rlog[1-pe^t]}$

$\frac{d}{dt}logM_Y(t)=\frac{rpe^t}{1-pe^t}$

$\mathbb{E}Y=\frac{d}{dt}logM_Y(t)\Big|_{t=0}=\frac{rpe^0}{1-pe^0}=\frac{rp}{1-p}$

$\operatorname{Var} Y =\frac{d^2}{d^2t}logM_Y(t)\Big|_{t=0}=\frac{(1-pe^t)rpe^t+pe^2rpe^t}{(1-pe^t)^2}\Big|_{t=0}=\frac{rp}{(1-p)^2}$

## Q3. The `dvisits` data comes from the Australian Health Survey of 1977--1978 and consist of 5190 single adults where young and old have been oversampled.

### (a) Make plots which show the relationship between the response variable, `doctorco`, and the potential predictors, `age` and `illness`.

**Answer:**

```{r}
library(faraway)
library(ggplot2)
```

```{r}
p <- ggplot(dvisits, aes(age, doctorco))
p + geom_jitter()
```

```{r}
p2 <- ggplot(dvisits, aes(illness, doctorco))
p2 + geom_jitter()
```

From the `age + doctorco` plot, we can see that the doctorco is higher
for low age and high age. It is less for middle age. From the
`illness + doctorco`, we can see that doctorco is higher for the middle
illness level. The doctorco is less for low illness levels and it the
least sparse for high illness level.

### (b) Combine the predictors `chcond1` and `chcond2` into a single three-level factor.Make an appropriate plot showing the relationship between this factor and the response. Comment.

**Answer:**

```{r}
# Creating the combined factor based on specified rules
dvisits$condition <- ifelse(dvisits$chcond1 == 1, 1,
                             ifelse(dvisits$chcond2 == 1, 2, 0))

# Converting the combined factor into a factor type with levels for clarity
dvisits$condition <- factor(dvisits$condition)

```

```{r}
ggplot(dvisits, aes(condition, doctorco)) + geom_boxplot()
```

`doctorco` in condition 2 is more spread out compare to conditon 0 and
condition 1. All three conditions have same median value at 0.

### (c) Build a Poisson regression model with `doctorco` as the response and `sex`, `age`, `agesq`, `income`, `levyplus`, `freepoor`, `freerepa`, `illness`, `actdays`, `hscore` and the three-level condition factor as possible predictor variables. Considering the deviance of this model, does this model fit the data?

**Answer:**

```{r}
mod <- glm(doctorco ~ sex + age + agesq + income + levyplus + freepoor + 
      illness + actdays + hscore + condition, dvisits, family = poisson)
```

```{r}
summary(mod)
```

The model has 4380.3 as residual deviance on 5178 degree of freedom. It
means there is no significant difference between current model and
saturated model. The model is a good fit to the data.

### (d) Plot the residuals and the fitted values --- why are there lines of observations on the plot? Make a QQ plot of the residuals and comment.

```{r}
library(tidyr)

dvisits %>%
  dplyr::mutate(devres  = residuals(mod, type = "deviance"), 
         linpred = predict(mod, type = "link")) %>%
  ggplot + 
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "Linear predictor", y = "Deviance residual")
```

```{r}
devres <- residuals(mod)
qqnorm(devres)
```

Since the model contains categorical predictors and count as the
outcome, this will likely result in groups and lines in the plot. In the
QQ plot, the deviations from the line in the tails indicate that the
residuals are not perfectly normally distributed.

### (e) Use a stepwise AIC-based model selection method. What sort of person would be predicted to visit the doctor the most under your selected model?

**Answer:**

```{r}
dvisits$sex = as.factor(dvisits$sex)
```

```{r}
library(gtsummary)
stats::step(mod, trace = TRUE, direction = "back") %>%
  tbl_regression() %>%
  bold_labels()
```

```{r}
mod2 <- glm(doctorco ~ sex + age + income + levyplus + freepoor + 
      illness + actdays + hscore + condition, dvisits, family = poisson)
```

```{r}
summary(mod2)
```

By observing the summary of the selected model, the person who is in
sex1, elder, lower income, lower freepoor, higher illness, more actdays,
and higher hscore are predicted to visit doctor the most.

### (f) For the last person in the dataset, compute the predicted probability distribution for their visits to the doctor, i.e., give the probability they visit 0, 1, 2, etc. times.

```{r}
last_person <- dvisits[nrow(dvisits), ]

lambda <- predict(mod2, newdata = last_person, type="response") 
```

```{r}
max_count <- 3  # This can be adjusted based on the data context
probabilities <- dpois(0:max_count, lambda)
probabilities
```

### (g) Tabulate the frequencies of the number of doctor visits. Compute the expected frequencies of doctor visits under your most recent model. Compare the observed with the expected frequencies and comment on whether it is worth fitting a zero-inflated count model.

```{r}
table(dvisits$doctorco)
```

```{r}
lambdas <- predict(mod2, newdata = dvisits, type="response") 

max_count <- 9  # This can be adjusted based on the data context

probabilities <- numeric(max_count + 1)

for ( i in lambdas) {
  probabilities <- probabilities + dpois(0:max_count, i)
}
probabilities
```

I think it is not worth fitting a zero-inflated count model because the
observed and expected frequencies are not very different from the above
comparison

### (h) Fit a comparable (Gaussian) linear model and graphically compare the fits. Describe how they differ.

```{r}
mod3 <- glm(doctorco ~ sex + age + income + levyplus + freepoor + 
      illness + actdays + hscore + condition, dvisits, family = gaussian)
```

```{r}
summary(mod3)
```

```{r}
linpred2  <- predict(mod2)
devres2 <- residuals(mod2)
dvisits %>%
  mutate(devres = devres2, linpred = linpred2) %>% 
  dplyr::group_by(cut(linpred, breaks = unique(quantile(linpred, (1:100)/101)))) %>%
  dplyr::summarize(devres = mean(devres), 
            linpred = mean(linpred)) %>%
  ggplot() +
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "Linear predictor", y = "Binned deviance residual")
```

```{r}
linpred3  <- predict(mod3)
devres3 <- residuals(mod3)
dvisits %>%
  mutate(devres = devres3, linpred = linpred3) %>% 
  dplyr::group_by(cut(linpred, breaks = unique(quantile(linpred, (1:100)/101)))) %>%
  dplyr::summarize(devres = mean(devres), 
            linpred = mean(linpred)) %>%
  ggplot() +
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "Linear predictor", y = "Binned deviance residual")
```

We can see the Gaussian model shows no specific pattern but the poisson
model shows an increasing pattern which might suggests it is not a good
fit for the data.

## Q4. Uniform association

For the uniform association when all two-way interactions are included,
i.e.,

$$
\log \mathbb{E}Y_{ijk} = \log p_{ijk} = \log n + \log p_i + \log p_j + \log p_k + \log p_{ij} + \log p_{ik} + \log p_{jk}.
$$

Proof the odds ratio (or log of odds ratio) across all stratum $k$

$$
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}}
$$

is a constant, i.e., the estimated effect of the interaction term "i:j"
in the uniform association model

$$
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}}=log\mathbb{E}Y_{11k}+log\mathbb{E}Y_{22k}-log\mathbb{E}Y_{12k}-log\mathbb{E}Y_{21k}\\
=\log p_{11}+\log p_{1k} +\log p_{1k}+\log p_{22}+log p_{2k}+\log p_{2k}-\log p_{12}-\log p_{1k}-\log p_{2k}-\log p_{21}-\log p_{2k}-\log p_{1k}\\
=\log p_{11}+log p_{22}-\log p_{12}-\log p_{21}
$$ Hence, the odd ratio across all stratum is a constant.
